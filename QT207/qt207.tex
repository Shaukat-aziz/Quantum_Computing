\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}

\begin{document}

\begin{center}
    \textbf{\huge QT207: Introduction to Quantum Computation}\\[1ex]
    \Large Assignment 1 \; | \; Shaukat Aziz (22171) \; | \; \today
\end{center}

% --- Problem 1 ---
\section*{\textbf{Problem 1}}
\textbf{Let $\{ |\varphi_1\rangle, \ldots, |\varphi_n\rangle \}$ and $\{ |\psi_1\rangle, \ldots, |\psi_n\rangle \}$ be orthonormal bases of a finite-dimensional vector space $V$. Define}
\[
U = \sum_{i=1}^n |\psi_i\rangle \langle \varphi_i|
\]
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item \textbf{Show that $U$ is unitary, i.e., $U^\dagger U = I$.}
    \begin{align*}
        U^\dagger U &= \left(\sum_{i=1}^n |\varphi_i\rangle \langle \psi_i|\right)\left(\sum_{j=1}^n |\psi_j\rangle \langle \varphi_j|\right) \\
        &= \sum_{i,j=1}^n |\varphi_i\rangle \langle \psi_i|\psi_j\rangle \langle \varphi_j| \\
        \intertext{Since $\langle \psi_i|\psi_j\rangle = \delta_{ij}$ (orthonormality),}
        &= \sum_{i=1}^n |\varphi_i\rangle \langle \varphi_i| \\
        &= I
    \end{align*}
    Thus, $U$ is unitary.

    \item \textbf{Show that $U|\varphi_j\rangle = |\psi_j\rangle$ for all $j$.}
    \begin{align*}
        U|\varphi_j\rangle &= \sum_{i=1}^n |\psi_i\rangle \langle \varphi_i|\varphi_j\rangle \\
        &= \sum_{i=1}^n |\psi_i\rangle \delta_{ij} \quad \text{(since $\langle \varphi_i|\varphi_j\rangle = \delta_{ij}$)}\\
        &= |\psi_j\rangle 
    \end{align*}
    So $U$ maps each $|\varphi_j\rangle$ to $|\psi_j\rangle$.
\end{enumerate}

% --- Problem 2 ---
\section*{\textbf{Problem 2}}
\textbf{Let $H$ be Hermitian and $U$ be unitary.}
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item \textbf{All eigenvalues of $U$ have unit modulus.}\\[1ex] 
    $U |u\rangle = \lambda|u\rangle \implies \langle u | U^\dagger = \langle u | \lambda^*$ where $\lambda \quad \& \quad \lambda^*$ are the eigenvalues of $U$ and $U^\dagger$ respectively.
    \begin{align*}
        \langle u \rvert U^\dagger U \lvert u \rangle &= \langle u | U^\dagger \lambda |u\rangle = \langle u | \lambda^* \lambda |u\rangle \\
        \langle u | I | u \rangle &= \langle u ||\lambda|^2 | u \rangle = |\lambda|^2 \langle u | u \rangle \\
        \langle u | u \rangle &= |\lambda|^2 \langle u | u \rangle 
        \implies |\lambda|^2 = 1 
    \end{align*}
    Since $\lambda$ is a complex number and the only condition is the magnitude = 1, we can write $\lambda = e^{i\phi}$ for some \textbf{real} $\phi$.

    \textbf{Each unitary $U$ can be written as $U = \exp(iH)$ for some Hermitian $H$.}\\[1ex]
    We will be using the following properties to prove the above :
    \begin{itemize}
        \item Unitary matrix $U$ is diagonalizable and can be written as $U = V^{-1} D V$ for some diagonal matrix $D$ and the Diagonal matrix contains all the eigen values of $U$.
        \item Exponent of a diagonal matrix is the diagonal matrix of the exponents, i.e., if $D = \mathrm{diag}(d_1, d_2, \ldots, d_n)$, then $\exp(D) = \mathrm{diag}(e^{d_1}, e^{d_2}, \ldots, e^{d_n})$.
        \item if $U =  V^{-1}D V$, then $\exp(U) = V^{-1} \exp(D) V$.
        \item For a Hermitian matrix $H^\dagger = H $ and eigenvalues of $H$ are real, diagonalizable via unitary tranformation i.e $H = W D' W^{-1}$ where D' is diagonal matrix with real eigenvalues.
    \end{itemize}
    Let U be a unitary matrix with eigen values $\lambda_j = e^{i\phi_j} \quad j = 1 \ldots n$.
\[
        \text{\textbf{Defining}}:\; D' = \mathrm{diag}(\phi_j) \quad \& \quad  
        D = \mathrm{diag}(e^{i\phi_j}) \implies exp(iD') = D
 \]
Then:
    \begin{align*}
        U &= V^{-1} D V = V^{-1} \exp(iD') V  \\
        \text{Let } H &= V^{-1} D' V  \implies exp(iH) = V^{-1} \exp(iD') V \\
        \implies U &= \exp(iH)
    \end{align*}
    The Above $H$ is Hermitian as the matrix is a diagonalizable matrix with real eigen values $\phi_j$.

    \item \textbf{Two eigenvectors of $U$ with different eigenvalues are orthogonal.}\\
    Let $U|u_1\rangle = \lambda_1|u_1\rangle$ and $U|u_2\rangle = \lambda_2|u_2\rangle$ with $\lambda_1 \neq \lambda_2$. Consider
    \begin{align*}
        \langle u_1|(U|u_2\rangle) &= \lambda_2 \langle u_1|u_2\rangle \\
        (\langle u_1|U)|u_2\rangle &= \lambda_1 \langle u_1|u_2\rangle \\
        \implies (\lambda_1 - \lambda_2)& \langle u_1|u_2\rangle = 0
    \end{align*}
    In the above we have use $\langle u_1|U = \langle u_1| \lambda_1$ because:
    \begin{align*}
        \langle u_1|U^\dagger &= \langle u_1| \lambda_1^* \\
        \implies \langle u_1|U^\dagger U &= \lambda_1^* \langle u_1|U \\
        \text{But } U^\dagger U = I &\implies \langle u_1|U = \frac{1}{\lambda_1^*} \langle u_1| \\
        \text{Since } |\lambda_1|^2 = 1 &\implies \lambda_1^* = \lambda_1^{-1} \\
        \implies \langle u_1|U = \lambda_1 \langle u_1| &
    \end{align*}
    Since $\lambda_1 \neq \lambda_2$, $\langle u_1|u_2\rangle = 0$. Thus, eigenvectors with distinct eigenvalues are orthogonal.

    \textbf{Similarly for Hermitian $H$:} If $H|v_1\rangle = \mu_1|v_1\rangle$, $H|v_2\rangle = \mu_2|v_2\rangle$, $\mu_1 \neq \mu_2$, then using $\langle v_1|H = \mu_1 \langle v_1|$ as $H = H^\dagger$:
    \begin{align*}
        \langle v_1 | (H | v_2 \rangle )&= \mu_2 \langle v_1 | v_2 \rangle \\
        \langle (v_1 | H )| v_2 \rangle &=  \mu_1 \langle v_1 | v_2 \rangle \\
        \implies (\mu_1 - \mu_2) &\langle v_1 | v_2 \rangle = 0
    \end{align*}
    So $(\mu_1 - \mu_2)\langle v_1 | v_2 \rangle = 0$. If $\mu_1 \neq \mu_2$, $\langle v_1 | v_2 \rangle = 0 \implies$ eigenvectors with distinct eigenvalues are orthogonal.
    \item \textbf{All columns of $U$ are orthonormal.}\\
    if $U_i$ is the column of a matrix U then 
    \begin{align*}
        U = \begin{bmatrix}
            | & | & & | \\
            U_1 & U_2 & \cdots & U_n \\
            | & | & & |
        \end{bmatrix}
    \end{align*}
    \begin{align*}
        I = (U^\dagger U)_{ij} = \sum_k (U^\dagger)_{ik} U_{kj} = \sum_k U_{ki}^* U_{kj} = \langle U_i | U_j \rangle = \delta_{ij}
    \end{align*}
    The above is orthonormality condition for column and for rows:
    \begin{align*}
                U = \begin{bmatrix}
            - & R_1 & - \\
            - & R_2 & - \\
            & \vdots & \\
            - & R_n & -
        \end{bmatrix}
    \end{align*}
    \begin{align*}
        I = (U U^\dagger)_{ij} = \sum_k U_{ik} U_{jk}^* = R_i R_j^* = \delta_{ij}
    \end{align*}
\end{enumerate}

% --- Problem 3 ---
\section*{\textbf{Problem 3}}
\textbf{Let $P$ be a linear operator on a finite-dimensional complex inner-product space $V$ with $P^2 = P$.}
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item \textbf{All eigenvalues of $P$ are $0$ or $1$.}
    \begin{align*}
        P|v\rangle &= \lambda|v\rangle \\
        P^2|v\rangle &= P(P|v\rangle) = P(\lambda|v\rangle) = \lambda P|v\rangle = \lambda^2|v\rangle \\
        \implies P^2|v\rangle &= P|v\rangle = \lambda|v\rangle \\
        \implies \lambda^2 = & \;\lambda  \implies \lambda = 0 \text{ or } 1
    \end{align*}
    \textbf{To prove P is diagonalizable:}
    We will be using a property of jordan matrix $J$ that if $J^2 = J$, then $J$ is diagonal matrix.
    Also, Any matrix can be expressed in terms of its Jordan form:
    \[ P = Y J Y^{-1} \implies P^2 = Y J Y^{-1} Y J Y^{-1} = Y J^2 Y^{-1} = Y J Y^{-1} = P \implies J^2 = J \]
    Since $J^2 = J$, $J$ is diagonal matrix. Hence $P$ is diagonalizable.
    \item \textbf{The complementary operator $Q = I - P$ is also a projector.}
    \begin{align*}
        Q^2 &= (I - P)(I - P) = I - 2P + P^2 \\
        &= I - 2P + P = I - P = Q
    \end{align*}

    \item \textbf{If $\{|u_i\rangle\}_{i=1}^r$ is an orthonormal set, then $P = \sum_{i=1}^r |u_i\rangle\langle u_i|$ is a projector.}
    \begin{align*}
        P^2 &= \left(\sum_{i=1}^r |u_i\rangle\langle u_i|\right)^2 \\
        &= \sum_{i,j=1}^r |u_i\rangle\langle u_i|u_j\rangle\langle u_j| \\
        &= \sum_{i,j=1}^r |u_i\rangle \delta_{ij} \langle u_j| \tag{\text{since $\langle u_i|u_j\rangle = \delta_{ij}$}}\\
        &= \sum_{i=1}^r |u_i\rangle\langle u_i| = P
    \end{align*}
\end{enumerate}

% --- Problem 4 ---
\section*{\textbf{Problem 4}}
\textbf{The Pauli matrices are:}
\[
\sigma_x = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}, \quad
\sigma_y = \begin{pmatrix} 0 & -i \\ i & 0 \end{pmatrix}, \quad
\sigma_z = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}
\]
They are Hermitian, unitary, and traceless. Prove:
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item \textbf{Squares and inverses and eigenvalues  : $\sigma_k^2 = I$ and $\sigma_k^{-1} = \sigma_k$ $k = x,y,z$.}
    Since $\sigma_k$ are Hermitian and Unitary, $\sigma_k^\dagger = \sigma_k = \sigma_k^{-1}$.
    \[ \implies \sigma_k = \sigma_k^{-1} \implies \sigma_k^2 = I \]
    To show the eigenvalues of a general Pauli matrix $\sigma_k$ are $\pm 1$, note that each Pauli matrix is $2 \times 2$, Hermitian, traceless, and unitary, and satisfies $\sigma_k^2 = I$. 

    Let $\lambda$ be an eigenvalue of $\sigma_k$, so $\sigma_k |v\rangle = \lambda |v\rangle$. Applying $\sigma_k$ again:
    \[
    \sigma_k^2 |v\rangle = \sigma_k (\sigma_k |v\rangle) = \sigma_k (\lambda |v\rangle) = \lambda \sigma_k |v\rangle = \lambda^2 |v\rangle
    \]
    But $\sigma_k^2 = I$, so $I|v\rangle = |v\rangle = \lambda^2 |v\rangle$. Thus,
    \[
    \lambda^2 = 1 \implies \lambda = \pm 1
    \]
    Therefore, any Pauli matrix $\sigma_k$ has eigenvalues $\pm 1$.
    \item \textbf{Commutators and anticommutators: $[\sigma_i, \sigma_j] = 2i\epsilon_{ijk}\sigma_k$ and $\{\sigma_i, \sigma_j\} = 2\delta_{ij}I$.}
    We will use the property : $ \sigma_i \sigma_j = - \sigma_j \sigma_i $ if $i \neq j$.
    \begin{align*}
        &\textbf{Commutators: } \\
        [\sigma_x, \sigma_y] &= \sigma_x\sigma_y - \sigma_y\sigma_x = i\sigma_z - (-i\sigma_z) = 2i\sigma_z \\
        [\sigma_y, \sigma_z] &= 2i\sigma_x, \quad [\sigma_z, \sigma_x] = 2i\sigma_y \\
        \implies [\sigma_i, \sigma_j] &= 2i\epsilon_{ijk}\sigma_k \\
        &\textbf{Anticommutators: }  \\
        \{\sigma_x, \sigma_y\} &= \sigma_x\sigma_y + \sigma_y\sigma_x = 0 \\
        \{\sigma_i, \sigma_i\} &= \sigma_i \sigma_i + \sigma_i \sigma_i = 2\sigma_i^2 = 2I \\
        \implies \{\sigma_i, \sigma_j\} &= \sigma_i\sigma_j + \sigma_j\sigma_i = 2\delta_{ij}I
    \end{align*}
    
    \item \textbf{Product identity: $\sigma_i\sigma_j = \delta_{ij}I + i\epsilon_{ijk}\sigma_k$.}
    \begin{align*}
        \sigma_i\sigma_j &= \frac{1}{2}\left([\sigma_i, \sigma_j] + \{\sigma_i, \sigma_j\}\right) \\
        &= \frac{1}{2}\left(2i\epsilon_{ijk}\sigma_k + 2\delta_{ij}I\right) \\
        &= \delta_{ij}I + i\epsilon_{ijk}\sigma_k
    \end{align*}
    For example, $\sigma_x\sigma_y = i\sigma_z$, $\sigma_y\sigma_x = -i\sigma_z$, and $\sigma_i\sigma_i = I$.

    \item \textbf{Vector identities for $\mathbf{a}, \mathbf{b}, \boldsymbol{\sigma} \in \mathbb{R}^3$:}\\
    Define:
    
    \begin{itemize}
        \item $\mathbf{a} = (a_1, a_2, a_3)$
        \item $\mathbf{b} = (b_1, b_2, b_3)$
        \item $\boldsymbol{\sigma} = (\sigma_x, \sigma_y, \sigma_z)$ (by definition)
    \end{itemize}
We will Prove first that $(\mathbf{a} \cdot \boldsymbol{\sigma})(\mathbf{b} \cdot \boldsymbol{\sigma}) = (\mathbf{a} \cdot \mathbf{b})I + i(\mathbf{a} \times \mathbf{b}) \cdot \boldsymbol{\sigma}$ :
    \begin{align*}
        (\mathbf{a} \cdot \boldsymbol{\sigma})(\mathbf{b} \cdot \boldsymbol{\sigma}) &= \sum_{i} a_i \sigma_i \sum_{j} b_j \sigma_j = \sum_{i,j} a_i b_j \sigma_i \sigma_j \\
        &= \sum_{i,j} a_i b_j \left(\delta_{ij}I + i\epsilon_{ijk}\sigma_k\right) \tag{$\because \sigma_i\sigma_j = \delta_{ij}I + i\epsilon_{ijk}\sigma_k$}\\
        &= \sum_{i} a_i b_i + i\sum_{i,j} a_i b_j \epsilon_{ijk}\sigma_k \\
        &= (\mathbf{a} \cdot \mathbf{b})I + i(\mathbf{a} \times \mathbf{b}) \cdot \boldsymbol{\sigma}
    \end{align*}
    Now taking $\mathbf{a} = \mathbf{b}$, we find:
    \begin{align*}
        (\mathbf{a} \cdot \boldsymbol{\sigma})^2 &= (\mathbf{a} \cdot \boldsymbol{\sigma})(\mathbf{a} \cdot \boldsymbol{\sigma}) \\
        &= (\mathbf{a} \cdot \mathbf{a})I + i(\mathbf{a} \times \mathbf{a}) \cdot \boldsymbol{\sigma} \\
        &= |\mathbf{a}|^2 I + 0 \\
        &= |\mathbf{a}|^2 I
    \end{align*}
\end{enumerate}

% --- Problem 5 ---
\section*{\textbf{Problem 5}}
\textbf{A density operator $\rho$ on a finite-dimensional Hilbert space $\mathcal{H}$ for an ensemble $\{p_i, |\psi_i\rangle\}$ is $\rho = \sum_i p_i |\psi_i\rangle\langle\psi_i|$. Prove:}
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item \textbf{$\rho$ is Hermitian, positive semidefinite, and $\mathrm{Tr}(\rho) = 1$.}
    \begin{align*}
        \rho^\dagger &= \sum_i p_i (|\psi_i\rangle\langle\psi_i|)^\dagger = \sum_i p_i |\psi_i\rangle\langle\psi_i| = \rho \tag{$\implies \rho \text{ is Hermitian}$}\\
        \langle\phi|\rho|\phi\rangle &= \sum_i p_i \langle\phi|\psi_i\rangle\langle\psi_i|\phi\rangle = \sum_i p_i |\langle\phi|\psi_i\rangle|^2 \geq 0 \tag{$\implies \rho \text{ is positive semidefinite}$}\\
        \mathrm{Tr}(\rho) &= \sum_i p_i \mathrm{Tr}(| \psi_i \rangle \langle \psi_i |) =\sum_i p_i \langle\psi_i|\psi_i\rangle = \sum_i p_i |\psi_i|^2 = 1 \tag{$\implies \mathrm{Tr}(\rho) = 1$}
    \end{align*}
    $\text{In the above: }p_i \geq 0 \text{ as it is probability and } |\langle\phi|\psi_i\rangle|^2  \geq 0 $, $\mathrm{Tr}(|a\rangle\langle a|) = \langle a|a\rangle$.
    \item \textbf{$0 \leq \mathrm{Tr}(\rho^2) \leq 1$ and $\rho$ represents a pure state $\iff \rho^2 = \rho \implies \mathrm{Tr}(\rho^2) = 1$.}
    \begin{align*}
        \mathrm{Tr}(\rho^2) &= \mathrm{Tr}\left(\sum_{i,j} p_i p_j |\psi_i\rangle\langle\psi_i|\psi_j\rangle\langle\psi_j|\right) \\
        &= \sum_{i,j} p_i p_j \mathrm{Tr}\left(|\psi_i\rangle\langle\psi_i|\psi_j\rangle\langle\psi_j|\right) \\
        &= \sum_{i,j} p_i p_j \langle\psi_i|\psi_j\rangle \mathrm{Tr}\left(|\psi_j\rangle\langle\psi_i|\right) \\
        &= \sum_{i,j} p_i p_j |\langle\psi_i|\psi_j\rangle|^2 \leq \sum_{i,j} p_i p_j = (\sum_i p_i)^2 = 1
    \end{align*}
    For pure states, $\rho = |\psi\rangle\langle\psi|$, $\rho^2 = \rho$, $\mathrm{Tr}(\rho^2) = 1$. For mixed states, $\mathrm{Tr}(\rho^2) < 1$. Like above, we can see that the inequality becomes strict for mixed states.

    \item \textbf{Spectral form: $\rho = \sum_k \lambda_k |\phi_k\rangle\langle\phi_k|$, $\lambda_k \geq 0$, $\sum_k \lambda_k = 1$. Probabilities are $\lambda_k$.}\\
    \begin{align*}
        \mathrm{Tr}(\rho^2) &= \mathrm{Tr}\left(\sum_{k,l} \lambda_k \lambda_l |\phi_k\rangle\langle\phi_k|\phi_l\rangle\langle\phi_l|\right) \nonumber \\
        &= \sum_{k,l} \lambda_k \lambda_l \delta_{k,l} \mathrm{Tr}\left(|\phi_l\rangle\langle\phi_k|\right) \tag{$\because \langle\phi_k|\phi_l\rangle = \delta_{k,l} \text{(orthonormality)}$} \nonumber \\
        &= \sum_k \lambda_k^2 \langle\phi_k|\phi_k\rangle \nonumber \\
        &= \sum_k \lambda_k^2 = 1 \text{ ($\because \rho^2 = \rho$) as orthonormal basis $|\phi_k\rangle$ is used for spectral decomposition}
    \end{align*}
    $ \rho $ here a pure state which was defined as $\rho = \sum_k |\phi_k\rangle\langle\phi_k|$ where $|\phi_k\rangle$ are orthonormal .

    \item \textbf{Expectation values: For observable $A$, $\langle A \rangle = \mathrm{Tr}(\rho A) = \sum_i p_i \langle\psi_i|A|\psi_i\rangle$.}
    \begin{align*}
        \langle A \rangle &= \langle\psi|A|\psi\rangle \text{ and } |\psi \rangle = \sum_i \sqrt{p_i} |\psi_i\rangle \tag{Definition}\\
        \langle A \rangle &= \sum_i p_i \langle\psi_i|A|\psi_i\rangle = \sum_i p_i \mathrm{Tr}(|\psi_i\rangle\langle\psi_i| A) = \mathrm{Tr}(\rho A) \\
    \end{align*}
\end{enumerate}

% --- Problem 6 ---
\section*{\textbf{Problem 6}}
\textbf{In finite dimensions, positivity of an operator $X$ means $\langle\psi|X|\psi\rangle\ge 0$ for all $|\psi\rangle$. Prove:}
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item \textbf{A positive operator is Hermitian.}
    \begin{proof}
    Write $A$ as $A=B+iC$ where $B=(A+A^\dagger)/2$ and $C=(A-A^\dagger)/(2i)$ are Hermitian. For any vector $|\psi\rangle$,
    \[
        \langle\psi|A|\psi\rangle = \langle\psi|B|\psi\rangle + i\langle\psi|C|\psi\rangle.
    \]
    By hypothesis the left-hand side is real and nonnegative for every $|\psi\rangle$, so its imaginary part must vanish for all $|\psi\rangle$, i.e.
    \[
        \langle\psi|C|\psi\rangle = 0 \qquad\text{for all } |\psi\rangle.
    \]
    Since $C$ is Hermitian, it admits a spectral decomposition:
    \[
        C = \sum_k c_k |\phi_k\rangle\langle\phi_k|
    \]
    where $c_k$ are real eigenvalues and $|\phi_k\rangle$ are orthonormal eigenvectors. Setting $|\psi\rangle = |\phi_k\rangle$ gives
    \[
        \langle\phi_k|C|\phi_k\rangle = c_k = 0 \qquad\text{for all } k.
    \]
    Therefore, all eigenvalues vanish and $C = 0$. Thus, $A = B$ is Hermitian.
    \end{proof}

    \item \textbf{For any linear operator $A$, the operator $A^\dagger A$ is positive and Hermitian.}
    \begin{proof}
    For any $|\psi\rangle$,
    \[
        \langle\psi|A^\dagger A|\psi\rangle = \langle A\psi|A\psi\rangle = \|A|\psi\rangle\|^2 \ge 0,
    \]
    so $A^\dagger A$ is positive. Moreover $(A^\dagger A)^\dagger = A^\dagger (A^\dagger)^\dagger = A^\dagger A$, so it is Hermitian. Consequently all eigenvalues of $A^\dagger A$ are real and nonnegative.
    \end{proof}
\end{enumerate}

% --- Problem 7 ---
\section*{\textbf{Problem 7}}
\textbf{For matrices $A \in \mathbb{C}^{m \times n}$, $C \in \mathbb{C}^{n \times p}$, $B \in \mathbb{C}^{k \times \ell}$, and $D \in \mathbb{C}^{\ell \times q}$, prove the mixed-product property:}
\[
    (A \otimes B)(C \otimes D) = (AC) \otimes (BD).
\]
\begin{proof}
Let $A \in \mathbb{C}^{m \times n}$, $C \in \mathbb{C}^{n \times p}$, $B \in \mathbb{C}^{k \times \ell}$, $D \in \mathbb{C}^{\ell \times q}$. The $(i,j)$-th block of $(A \otimes B)(C \otimes D)$ is:
\[
    \sum_{r=1}^n (a_{ir}B)(c_{rj}D).
\]
Since scalars commute with matrices, we can regroup:
\[
    \sum_{r=1}^n (a_{ir}c_{rj})(BD).
\]
Thus the $(i,j)$-th block of $(A \otimes B)(C \otimes D)$ is:
\[
    \left(\sum_{r=1}^n a_{ir}c_{rj}\right) BD = (AC)_{ij} BD.
\]
But this is exactly the $(i,j)$-th block of $(AC) \otimes (BD)$:
\[
    (AC) \otimes (BD) = \begin{bmatrix}
        (AC)_{11}(BD) & \cdots & (AC)_{1p}(BD) \\
        \vdots & \ddots & \vdots \\
        (AC)_{m1}(BD) & \cdots & (AC)_{mp}(BD)
    \end{bmatrix}.
\]
Therefore,
\[
    (A \otimes B)(C \otimes D) = (AC) \otimes (BD).
\]
\end{proof}

\end{document}
